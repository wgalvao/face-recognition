{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face-Recognition\n",
    "Este notebook tem como objetivo fornecer um meio de avaliar os modelos presentes no Face-Recognition utilizando o dataset LFW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações e Inicialização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../../src/models/face-recognition')\n",
    "from evaluate import eval, extract_deep_features, eval_accuracy, find_best_threshold, calculate_tar_far_frr\n",
    "from models import (\n",
    "    sphere20,\n",
    "    sphere36,\n",
    "    sphere64,\n",
    "    MobileNetV1,\n",
    "    MobileNetV2,\n",
    "    mobilenet_v3_small,\n",
    "    mobilenet_v3_large\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../../src/models/face-recognition')\n",
    "from evaluate import eval, extract_deep_features, eval_accuracy, find_best_threshold, calculate_tar_far_frr\n",
    "from models import (\n",
    "    sphere20,\n",
    "    sphere36,\n",
    "    sphere64,\n",
    "    MobileNetV1,\n",
    "    MobileNetV2,\n",
    "    mobilenet_v3_small,\n",
    "    mobilenet_v3_large\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecionar Modelo e Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "1. sphere20\n",
      "2. sphere36\n",
      "3. sphere64\n",
      "4. mobilenetv1\n",
      "5. mobilenetv2\n",
      "6. mobilenetv3_small\n",
      "7. mobilenetv3_large\n",
      "\n",
      "Selected model: mobilenetv3_large\n",
      "Model checkpoint: ../../src/models/face-recognition/weights/mobilenetv3_large_5.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Modelos Disponíveis\n",
    "available_models = {\n",
    "    \"sphere20\": sphere20,\n",
    "    \"sphere36\": sphere36,\n",
    "    \"sphere64\": sphere64,\n",
    "    \"mobilenetv1\": MobileNetV1,\n",
    "    \"mobilenetv2\": MobileNetV2,\n",
    "    \"mobilenetv3_small\": mobilenet_v3_small,\n",
    "    \"mobilenetv3_large\": mobilenet_v3_large\n",
    "}\n",
    "\n",
    "print(\"Available models:\")\n",
    "for i, model_name in enumerate(available_models.keys(), 1):\n",
    "    print(f\"{i}. {model_name}\")\n",
    "\n",
    "# Selecionar peso\n",
    "model_name = \"mobilenetv3_large\"\n",
    "checkpoint_name = \"mobilenetv3_large_5\"\n",
    "embedding_dim = 512\n",
    "model_path = f\"../../src/models/face-recognition/weights/{checkpoint_name}.ckpt\"\n",
    "\n",
    "print(f\"\\nSelected model: {model_name}\")\n",
    "print(f\"Model checkpoint: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete checkpoint from epoch 4\n",
      "Model loaded from: ../../src/models/face-recognition/weights/mobilenetv3_large_5.ckpt\n",
      "Model ready for evaluation\n"
     ]
    }
   ],
   "source": [
    "model_class = available_models[model_name]\n",
    "model = model_class(embedding_dim=embedding_dim)\n",
    "\n",
    "if model_path and os.path.exists(model_path):\n",
    "    # Carregar checkpoint completo\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    \n",
    "    # Verificar se é um checkpoint completo ou apenas pesos\n",
    "    if 'model' in checkpoint:\n",
    "        # É um checkpoint completo - extrair apenas o modelo\n",
    "        model_state_dict = checkpoint['model']\n",
    "        print(f\"Loading complete checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "    else:\n",
    "        # São apenas os pesos do modelo\n",
    "        model_state_dict = checkpoint\n",
    "        print(\"Loading model weights only\")\n",
    "    \n",
    "    model.load_state_dict(model_state_dict)\n",
    "    print(f\"Model loaded from: {model_path}\")\n",
    "else:\n",
    "    print(f\"Warning: Checkpoint not found at {model_path}\")\n",
    "    print(\"Evaluating with random weights...\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Model ready for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar Caminho do Dataset LFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFW dataset path: ../../data/raw/lfw\n",
      "✓ Path exists\n"
     ]
    }
   ],
   "source": [
    "lfw_dataset_path = \"../../data/raw/lfw\"  # Adjuste conforme necessário\n",
    "\n",
    "print(f\"LFW dataset path: {lfw_dataset_path}\")\n",
    "if os.path.exists(lfw_dataset_path):\n",
    "    print(\"✓ Path exists\")\n",
    "else:\n",
    "    print(\"✗ Warning: Path does not exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo das Métricas no LFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on LFW dataset...\n",
      "==================================================\n",
      "LFW - Avaliacao Simplificada (Somente Pares Positivos):\n",
      "Similaridade Media: 0.6256 | Desvio Padrao: 0.1339\n",
      "==================================================\n",
      "\n",
      "Evaluation complete!\n",
      "Average Similarity Score: 0.6256\n",
      "LFW Accuracy: 0.9650\n",
      "Total pairs evaluated: 3000\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting evaluation on LFW dataset...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Executar avaliação com as novas métricas\n",
    "similarity_score, predictions, metrics = eval(\n",
    "    model, \n",
    "    model_path=None, \n",
    "    lfw_root=lfw_dataset_path, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Calcular acurácia no notebook\n",
    "if len(predictions) > 0:\n",
    "    accuracy = eval_accuracy(predictions, 0.35)\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nEvaluation complete!\")\n",
    "    print(f\"Average Similarity Score: {similarity_score:.4f}\")\n",
    "    print(f\"LFW Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"\\nBiometric Metrics at threshold 0.35:\")\n",
    "    print(f\"TAR (True Acceptance Rate): {metrics['TAR']:.4f}\")\n",
    "    print(f\"FAR (False Acceptance Rate): {metrics['FAR']:.4f}\")\n",
    "    print(f\"FRR (False Rejection Rate): {metrics['FRR']:.4f}\")\n",
    "    print(f\"\\nTotal pairs evaluated: {len(predictions)}\")\n",
    "else:\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nEvaluation complete!\")\n",
    "    print(f\"Average Similarity Score: {similarity_score:.4f}\")\n",
    "    print(f\"No valid pairs found for accuracy calculation\")\n",
    "    print(f\"Total pairs evaluated: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise em Diferentes Tresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(predictions) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANÁLISE DE MÉTRICAS EM DIFERENTES THRESHOLDS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Testar diferentes thresholds\n",
    "    test_thresholds = [0.25, 0.30, 0.35, 0.40, 0.45, 0.50]\n",
    "    \n",
    "    print(f\"\\n{'Threshold':<12} {'TAR':<10} {'FAR':<10} {'FRR':<10} {'Accuracy':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for thresh in test_thresholds:\n",
    "        metrics_at_thresh = calculate_tar_far_frr(predictions, thresh)\n",
    "        acc_at_thresh = eval_accuracy(predictions, thresh)\n",
    "        \n",
    "        print(f\"{thresh:<12.2f} {metrics_at_thresh['TAR']:<10.4f} \"\n",
    "              f\"{metrics_at_thresh['FAR']:<10.4f} {metrics_at_thresh['FRR']:<10.4f} \"\n",
    "              f\"{acc_at_thresh:<10.4f}\")\n",
    "    \n",
    "    # Encontrar o melhor threshold (EER - Equal Error Rate)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MELHOR THRESHOLD (Minimizando EER)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    best_threshold, best_metrics = find_best_threshold(predictions)\n",
    "    best_accuracy = eval_accuracy(predictions, best_threshold)\n",
    "    \n",
    "    print(f\"\\nBest Threshold: {best_threshold:.4f}\")\n",
    "    print(f\"TAR: {best_metrics['TAR']:.4f}\")\n",
    "    print(f\"FAR: {best_metrics['FAR']:.4f}\")\n",
    "    print(f\"FRR: {best_metrics['FRR']:.4f}\")\n",
    "    print(f\"Accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"EER (|FAR - FRR|): {abs(best_metrics['FAR'] - best_metrics['FRR']):.4f}\")\n",
    "else:\n",
    "    print(\"No predictions to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise do Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Statistics:\n",
      "Mean: 0.6256\n",
      "Std: 0.1339\n",
      "Min: 0.1134\n",
      "Max: 0.9901\n",
      "Median: 0.6407\n"
     ]
    }
   ],
   "source": [
    "if len(predictions) > 0:\n",
    "    similarities = predictions[:, 2].astype(float)\n",
    "    \n",
    "    # Estatísticas\n",
    "    print(\"Similarity Statistics:\")\n",
    "    print(f\"Mean: {np.mean(similarities):.4f}\")\n",
    "    print(f\"Std: {np.std(similarities):.4f}\")\n",
    "    print(f\"Min: {np.min(similarities):.4f}\")\n",
    "    print(f\"Max: {np.max(similarities):.4f}\")\n",
    "    print(f\"Median: {np.median(similarities):.4f}\")\n",
    "else:\n",
    "    print(\"No predictions to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliar Vários Modelos (Batch Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_evaluate = [\n",
    "    (\"sphere20\", \"../../src/models/face-recognition/weights/sphere20_mcp.ckpt\"),\n",
    "    (\"mobilenetv1\", \"../../src/models/face-recognition/weights/mobilenetv1_mcp.ckpt\"),\n",
    "    (\"mobilenetv2\", \"../../src/models/face-recognition/weights/mobilenetv2_mcp.ckpt\"),\n",
    "    # Adicione mais modelos conforme necessário\n",
    "]\n",
    "\n",
    "results = []\n",
    "print(\"Batch Evaluation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for model_name, checkpoint_path in models_to_evaluate:\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Skipping {model_name}: checkpoint not found at {checkpoint_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    # Inicializar modelo\n",
    "    model_class = available_models[model_name]\n",
    "    model = model_class(embedding_dim=512).to(device)\n",
    "    \n",
    "    # CORREÇÃO: Passar o checkpoint_path para carregar o modelo treinado\n",
    "    score, preds = eval(model, model_path=checkpoint_path, lfw_root=lfw_dataset_path, device=device)\n",
    "    \n",
    "    results.append({\n",
    "        'model': model_name,\n",
    "        'score': score,\n",
    "        'num_pairs': len(preds)\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"{result['model']:20s} | Score: {result['score']:.4f} | Pairs: {result['num_pairs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspeção de Predição de Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(predictions) > 0:\n",
    "    print(\"Sample Predictions (first 5):\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for i in range(min(5, len(predictions))):\n",
    "        path1, path2, similarity, gt = predictions[i]\n",
    "        print(f\"\\nPair {i+1}:\")\n",
    "        print(f\"  Image 1: {path1}\")\n",
    "        print(f\"  Image 2: {path2}\")\n",
    "        print(f\"  Similarity: {float(similarity):.4f}\")\n",
    "        print(f\"  Ground Truth: {'Same' if gt == '1' else 'Different'}\")\n",
    "else:\n",
    "    print(\"No predictions to display.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
